{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3477/1104101482.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf')\n"
     ]
    }
   ],
   "source": [
    "# standard libs\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax as NumPy on accelerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using jax 0.4.37\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(\"Using jax\", jax.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = jnp.zeros((2, 5), dtype=jnp.float32)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "b = jnp.arange(6)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CudaDevice(id=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "b_cpu = jax.device_get(b)\n",
    "print(b_cpu.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device put: <class 'jaxlib.xla_extension.ArrayImpl'> on cuda:0\n"
     ]
    }
   ],
   "source": [
    "b_gpu = jax.device_put(b_cpu)\n",
    "print(f\"Device put: {b_gpu.__class__} on {b_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 0,  2,  4,  6,  8, 10], dtype=int32), CudaDevice(id=0))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_cpu + b_gpu, (b_cpu + b_gpu).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [JAX - Asynchronous Dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = jnp.matmul(b, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutable tensors: Pure Function Language\n",
    "\n",
    "However, we said that JAX is very efficient. Isn’t creating a new array in this case the opposite? While it is indeed less efficient, it can made much more efficient with JAX’s just-in-time compilation. The compiler can recognize unnecessary array duplications, and replace them with in-place operations again. More on the just-in-time compilation later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array: [0 1 2 3 4 5]\n",
      "Changed Array: [1 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "b_new = b.at[0].set(1)  # returns a new array\n",
    "print(\"Original Array:\", b)\n",
    "print(\"Changed Array:\", b_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Random Numbers in JAX: For Pure Function\n",
    "\n",
    "In libraries like NumPy and PyTorch, the random number generator are controlled by a seed, which we set initially to obtain the same samples every time we run the code (this is why the numbers are not truly random, hence “pseudo”-random). However, if we call np.random.normal() 5 times consecutively, we will get 5 different numbers since every execution changes the state/seed of the pseudo random number generation (PRNG). In JAX, if we would try to generate a random number with this approach, a function creating pseudo-random number would have an effect outside of it. To prevent this, JAX takes a different approach by explicitly passing and iterating the PRNG state. First, let’s create a PRNG for the seed 42:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX - Random number 1: -0.18471177\n",
      "JAX - Random number 2: -0.18471177\n",
      "NumPy - Random number 1: 0.4967141530112327\n",
      "NumPy - Random number 2: -0.13826430117118466\n"
     ]
    }
   ],
   "source": [
    "# A non-desirable way of generating pseudo-random numbers...\n",
    "jax_random_number_1 = jax.random.normal(rng)\n",
    "jax_random_number_2 = jax.random.normal(rng)\n",
    "print(\"JAX - Random number 1:\", jax_random_number_1)\n",
    "print(\"JAX - Random number 2:\", jax_random_number_2)\n",
    "\n",
    "# Typical random numbers in NumPy\n",
    "np.random.seed(42)\n",
    "np_random_number_1 = np.random.normal()\n",
    "np_random_number_2 = np.random.normal()\n",
    "print(\"NumPy - Random number 1:\", np_random_number_1)\n",
    "print(\"NumPy - Random number 2:\", np_random_number_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you want to split the PRNG key every time before generating a pseudo-number, to prevent accidentally obtaining the exact same numbers (for instance, sampling the exact same dropout mask every time you run the network makes dropout itself quite useless…)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX new - Random number 1: 0.107961535\n",
      "JAX new - Random number 2: -1.2226542\n"
     ]
    }
   ],
   "source": [
    "rng, subkey1, subkey2 = jax.random.split(rng, num=3)  # We create 3 new keys\n",
    "jax_random_number_1 = jax.random.normal(subkey1)\n",
    "jax_random_number_2 = jax.random.normal(subkey2)\n",
    "print(\"JAX new - Random number 1:\", jax_random_number_1)\n",
    "print(\"JAX new - Random number 2:\", jax_random_number_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Transformations with Jaxpr\n",
    "\n",
    "The most important difference, and in some sense the root of all the rest, is that JAX is designed to be functional, as in functional programming. The reason behind this is that the kinds of program transformations that JAX enables are much more feasible in functional-style programs. […] The important feature of functional programming to grok when working with JAX is very simple: don’t write code with side-effects.\n",
    "\n",
    "Essentially, we want to write our main code of JAX in functions that do not affect anything else besides its outputs. For instance, we do not want to change input arrays in-place, or access global variables. While this might seem limiting at first, you get used to this quite quickly and most JAX functions that need to fulfill these constraints can be written this way without problems. Note that not all possible functions in training a neural network need to fulfill the constraints. For instance, loading or saving of models, the logging, or the data generation can be done in naive functions. Only the network execution, which we want to do very efficiently on our accelerator (GPU or TPU), should strictly follow these constraints.\n",
    "\n",
    "``jaxpr``, conceptually, you can think of any operation that JAX does on a function, as first trace-specializing the Python function to be transformed into a small and well-behaved intermediate form. This means that we check which operations are performed on which array, and what shapes the arrays are. Based on this representation, JAX then interprets the function with transformation-specific interpretation rules, which includes automatic differentiation or compiling a function in XLA to efficiently use the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0. 1. 2.]\n",
      "Output: 12.666667\n"
     ]
    }
   ],
   "source": [
    "def simple_graph(x):\n",
    "    x = x + 2\n",
    "    x = x**2\n",
    "    x = x + 3\n",
    "    y = x.mean()\n",
    "    return y\n",
    "\n",
    "\n",
    "inp = jnp.arange(3, dtype=jnp.float32)\n",
    "print(\"Input:\", inp)\n",
    "print(\"Output:\", simple_graph(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3]. let\n",
       "    b:f32[3] = add a 2.0\n",
       "    c:f32[3] = integer_pow[y=2] b\n",
       "    d:f32[3] = add c 3.0\n",
       "    e:f32[] = reduce_sum[axes=(0,)] d\n",
       "    f:f32[] = div e 3.0\n",
       "  in (f,) }"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(simple_graph)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A jaxpr representation follows the structure:\n",
    "```\n",
    "jaxpr ::= { lambda Var* ; Var+.\n",
    "            let Eqn*\n",
    "            in  [Expr+] }\n",
    "```\n",
    "\n",
    "where `Var*` are constants and `Var+` are input arguments. In the cell above, this is `a:f32[3]`, i.e. an array of shape 3 with type `jnp.float32` (`inp`). The list of equations, `Eqn*`, define the intermediate results of the function. You can see that each operation in `simple_graph` is translated to a corresponding equation, like `x = x + 2` is translated to `b:f32[3] = add a 2.0`. Furthermore, you see the specialization of the operations on the input shape, like `x.mean()` being replacing in `e` and `f` with summing and dividing by 3. Finally, `Expr+` in the jaxpr representation are the outputs of the functions. In the example, this is `f`, i.e. the final result of the function. Based on these atomic operations, JAX offers all kind of function transformations, of which we will discuss the most important ones later in this section. Hence, you can consider the jaxpr representation is an intermediate compilation stage of JAX. What happens if we actually try to look at the jaxpr representation of a function with side-effect? Let’s consider the following function, which, as an illustrative example, appends the input to a global list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3]. let\n",
       "    b:f32[3] = integer_pow[y=2] a\n",
       "    c:f32[] = reduce_sum[axes=(0,)] b\n",
       "    d:f32[] = sqrt c\n",
       "  in (d,) }"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_list = []\n",
    "\n",
    "\n",
    "# invalid function with side-effect\n",
    "def norm(x):\n",
    "    global_list.append(x)  # unavailable\n",
    "    x = x**2\n",
    "    n = x.sum()\n",
    "    n = jnp.sqrt(n)\n",
    "    return n\n",
    "\n",
    "jax.make_jaxpr(norm)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "In frameworks like PyTorch with a dynamic computation graph, we would compute the gradients based on the loss tensor itself, e.g. by calling loss.backward(). However, JAX directly works with functions. Instead of backpropagating gradients through tensors, JAX takes as input a function, and outputs another function which directly calculates the gradients for it. While this might seem quite different to what you are used to from other frameworks, it is quite intuitive: your gradient of parameters is really a function of parameters and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient [1.3333334 2.        2.6666667]\n"
     ]
    }
   ],
   "source": [
    "grad_function = jax.grad(simple_graph)\n",
    "gradients = grad_function(inp)\n",
    "print('Gradient', gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3]. let\n",
       "    b:f32[3] = add a 2.0\n",
       "    c:f32[3] = integer_pow[y=2] b\n",
       "    d:f32[3] = integer_pow[y=1] b\n",
       "    e:f32[3] = mul 2.0 d\n",
       "    f:f32[3] = add c 3.0\n",
       "    g:f32[] = reduce_sum[axes=(0,)] f\n",
       "    _:f32[] = div g 3.0\n",
       "    h:f32[] = div 1.0 3.0\n",
       "    i:f32[3] = broadcast_in_dim[\n",
       "      broadcast_dimensions=()\n",
       "      shape=(3,)\n",
       "      sharding=None\n",
       "    ] h\n",
       "    j:f32[3] = mul i e\n",
       "  in (j,) }"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(grad_function)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(12.666667, dtype=float32),\n",
       " Array([1.3333334, 2.       , 2.6666667], dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get both grad and value\n",
    "val_grad_function = jax.value_and_grad(simple_graph)\n",
    "val_grad_function(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More topics like multi-input grad and PyTree will be mentioned later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jit: Just-In-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
