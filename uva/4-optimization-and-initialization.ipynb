{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Optimization and Initialization\n",
    "In this tutorial, we will review techniques for optimization and initialization of neural networks. When increasing the depth of neural networks, there are various challenges we face. Most importantly, we need to have a stable gradient flow through the network, as otherwise, we might encounter vanishing or exploding gradients. This is why we will take a closer look at the following concepts: initialization and optimization.\n",
    "\n",
    "In the first half of the notebook, we will review different initialization techniques, and go step by step from the simplest initialization to methods that are nowadays used in very deep networks. In the second half, we focus on optimization comparing the optimizers SGD, SGD with Momentum, and Adam.\n",
    "\n",
    "Let’s start with importing our standard libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "from typing import Any, Sequence, Callable, NamedTuple, Optional, Tuple\n",
    "\n",
    "PyTree = Any  # Type definition for PyTree, for readability\n",
    "import pickle\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "# Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('svg', 'pdf')  # SVG and PDF are for export\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Jax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax.tree_util import tree_map\n",
    "\n",
    "# Seeding for random operations\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "# Flax\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import checkpoints, train_state\n",
    "\n",
    "# Optax\n",
    "import optax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path variables `DATASET_PATH` and `CHECKPOINT_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the datasets are/shoule be downloaded.\n",
    "DATASET_PATH = './data'\n",
    "# Path to the folder where the pretrained models are saved.\n",
    "CHECKPOINT_PATH = './saved_models/tutorial4'\n",
    "\n",
    "# Verifying the device that will be used throughout this notebook\n",
    "print('Device:', jax.devices()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the notebook, we will train models using three different optimizers. The pretrained models for those are downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "Throughout this notebook, we will use a deep fully connected network, similar to our previous tutorial. We will apply the network to FashionMNIST. We start by loading the FashionMNIST dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def image_to_numpy(image):\n",
    "    \"\"\"Transformations applied on each image =>\n",
    "    bring them into a numpy array and normalize to mean 0 and std 1.\n",
    "    \"\"\"\n",
    "    img = np.array(image, dtype=np.float32)\n",
    "    img = (img / 255.0 - 0.2860) / 0.3530  # TODO: WTH\n",
    "    return img\n",
    "\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    \"\"\"Stack the batch elements as numpy arrays. By default, PyTorch stacks them as tensors. For JAX, we need numpy arrays.\"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "\n",
    "# Load the FashionMNIST dataset. We need to split the dataset into training and validation sets.\n",
    "train_dataset = FashionMNIST(\n",
    "    root=DATASET_PATH, train=True, transform=image_to_numpy, download=True\n",
    ")\n",
    "train_set, val_set = data.random_split(\n",
    "    train_dataset, [50000, 10000], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Load the test set\n",
    "test_set = FashionMNIST(\n",
    "    root=DATASET_PATH, train=False, transform=image_to_numpy, download=True\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = data.DataLoader(\n",
    "    train_set, batch_size=1024, shuffle=False, drop_last=False, collate_fn=numpy_collate\n",
    ")\n",
    "val_loader = data.DataLoader(\n",
    "    val_set, batch_size=1024, shuffle=False, drop_last=False, collate_fn=numpy_collate\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    test_set, batch_size=1024, shuffle=False, drop_last=False, collate_fn=numpy_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the previous tutorial, we have changed the parameters of the normalization transformation in `image_to_numpy`. The normalization is now designed to give us an expected mean of 0 and a standard deviation of 1 across pixels. This will be particularly relevant for the discussion about initialization we will look at below, and hence we change it here. It should be noted that in most classification tasks, both normalization techniques (between -1 and 1 or mean 0 and stddev 1) have shown to work well. We can calculate the normalization parameters by determining the mean and standard deviation on the original images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.28604060411453247\n",
      "Std: 0.3530242443084717\n"
     ]
    }
   ],
   "source": [
    "print('Mean:', (train_dataset.data.float() / 255.0).mean().item())\n",
    "print('Std:', (train_dataset.data.float() / 255.0).std().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the transformation by looking at the statistics of a single batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.008\n",
      "Std:  1.009\n",
      "Maximun:  2.022\n",
      "Minimum: -0.810\n"
     ]
    }
   ],
   "source": [
    "imgs, _ = next(iter(train_loader))\n",
    "print(f'Mean: {imgs.mean().item(): 5.3f}')\n",
    "print(f'Std: {imgs.std().item(): 5.3f}')\n",
    "print(f'Maximun: {imgs.max().item(): 5.3f}')\n",
    "print(f'Minimum: {imgs.min().item(): 5.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the maximum and minimum are not 1 and -1 anymore, but shifted towards the positive values. This is because FashionMNIST contains a lot of black pixels, similar to MNIST.\n",
    "\n",
    "Next, we create a linear neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition\n",
    "class BaseNetwork(nn.Module):\n",
    "    features: int\n",
    "    act_fn: Callable\n",
    "    num_classes: int = 10\n",
    "    hidden_sizes: Sequence[int] = (512, 256, 256, 128)\n",
    "    kernel_init: Callable = nn.linear.default_kernel_init\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.typing.ArrayLike, return_activations: bool = False):\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten the input image to a vector.\n",
    "        # We collect all activations throughout the network for later visualizations\n",
    "        # Remember that in jitted functions, unused tensors will anyways be removed.\n",
    "        activcations = []\n",
    "        for hd in self.hidden_sizes:\n",
    "            x = nn.Dense(hd, kernel_init=self.kernel_init)(x)\n",
    "            activcations.append(x)\n",
    "            x = self.act_fn(x)\n",
    "            activcations.append(x)\n",
    "        x = nn.Dense(self.num_classes, kernel_init=self.kernel_init)(x)\n",
    "        activcations.append(x)\n",
    "        return x if not return_activations else (x, activcations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the activation functions, we make use of JAX’s and Flax’s library instead of implementing ourselves. However, we also define an `Identity` activation function. Although this activation function would significantly limit the network’s modeling capabilities, we will use it in the first steps of our discussion about initialization (for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_fn_by_name = {\n",
    "    'tanh': jax.nn.tanh,\n",
    "    'relu': jax.nn.relu,\n",
    "    'identity': lambda x: x,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a few plotting functions that we will use for our discussions. These functions help us to\n",
    "\n",
    "1. visualize the weight/parameter distribution inside a network;\n",
    "\n",
    "2. visualize the gradients that the parameters at different layers receive;\n",
    "\n",
    "3. the activations, i.e. the output of the linear layers. \n",
    "\n",
    "The detailed code is not important, but feel free to take a closer look if interested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
